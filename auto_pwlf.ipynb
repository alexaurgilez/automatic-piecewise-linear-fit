{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cda510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the cumulative displacement time series\n",
    "cum_lm1 = pd.read_excel('FILE.xlsx', index_col='date', parse_dates=True) #excel file with headers: \"datenumber\"; \"POINT_ID's\"\n",
    "cum_lm = cum_lm1[:]\n",
    "\n",
    "keyscoord_lm = list(cum_lm.keys())\n",
    "print(keyscoord_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7bf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hampel_filter_forloop(input_series, window_size, n_sigmas=2):\n",
    "    \n",
    "    n = len(input_series)\n",
    "    new_series = input_series.copy()\n",
    "    k = 1.4826 # scale factor for Gaussian distribution\n",
    "    count_out = 0\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    # possibly use np.nanmedian \n",
    "    for i in range((window_size),(n - window_size)):\n",
    "        x0 = np.median(input_series[(i - window_size):(i + window_size)])\n",
    "        S0 = k * np.median(np.abs(input_series[(i - window_size):(i + window_size)] - x0))\n",
    "        if (np.abs(input_series[i] - x0) > n_sigmas * S0):\n",
    "            new_series[i] = np.nan\n",
    "            count_out = count_out +1\n",
    "            indices.append(i)\n",
    "    \n",
    "    return new_series, indices, count_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49386bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final1 = []\n",
    "n_out_all = []\n",
    "for j in range(np.array(keyscoord_lm).size-1):  \n",
    "    y = np.array(cum_lm[keyscoord_lm[j]]) \n",
    "    x = np.array(cum_lm.datenumber)\n",
    "    \n",
    "    res, detected_outliers, n_out = hampel_filter_forloop(y, 7)\n",
    "    n_out_all = np.append(n_out_all, n_out)\n",
    "    y_final1 = pd.DataFrame(np.append(y_final1, res)) #array of all ts pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a matrix of the time series from the single vector from before\n",
    "auxiliar1 = pd.DataFrame()\n",
    "for i in range(np.array(keyscoord_lm).size-1):\n",
    "    w = np.array(y_final1[i*cum_lm.shape[0]:(i+1)*cum_lm.shape[0]])\n",
    "    auxiliar1 = pd.concat([auxiliar1, pd.DataFrame(w)], axis=1)\n",
    "\n",
    "auxiliar1.columns = keyscoord_lm[:-1]\n",
    "auxiliar1['datenumber'] = np.array(cum_lm.datenumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,30))\n",
    "for i in range(10):\n",
    "    y = cum_lm[keyscoord_lm[i+6]] #values of cumulative curve\n",
    "    x = pd.to_datetime('1899-12-30') + pd.to_timedelta(cum_lm.datenumber,'D')\n",
    "    y_calc = auxiliar1[keyscoord_lm[i+6]]\n",
    "    plt.subplot(10, 2, i + 1)\n",
    "    plt.plot(x, y, 'mD', label='data', markersize=3)\n",
    "    plt.plot(x, y_calc, 'ok', label='data', markersize=4)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Displacement (mm)')\n",
    "    plt.xticks(rotation=25)\n",
    "    plt.title(str(keyscoord_lm[i+6]))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyscoord_auxilar = list(auxiliar1.keys())\n",
    "print(keyscoord_auxilar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6849a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pwlf==2.0.4\n",
    "import pwlf\n",
    "#+ (((2*k)*(k+1))/(n-k-1))\n",
    "def aicc(ssr1, k, n):\n",
    "    '''Returns the value of AIC of the model'''\n",
    "    return n * np.log(ssr1/n) + 2 * k + (((2*k)*(k+1))/(n-k-1))\n",
    "\n",
    "def aicc_C(aic_new, diff):\n",
    "    '''Returns a dataframe containing T/F values of diff slopes on every breakpoint when AIC complies/not complies'''\n",
    "    diff1 = np.empty_like(diff)\n",
    "    for i in range(len(diff1)):\n",
    "        diff1[i] = aic_new\n",
    "    return pd.DataFrame(diff1)\n",
    "\n",
    "def ssres(yfit, y):\n",
    "    return np.sum((yfit - y) ** 2)\n",
    "\n",
    "def BP_LOC(diff, thres, SE_BP):\n",
    "    '''First criterion to determine significant breakpoint locations\n",
    "    diff: column of the differences of consecutive slopes\n",
    "    thres: threshold above which the standard error of the breakpoints is too high to be significant\n",
    "    SE_BP: standard errors of the (inner)breakpoints of the model'''\n",
    "    diff1 = np.empty_like(diff)\n",
    "    for i in range(len(diff1)):\n",
    "        threshold = thres\n",
    "        se_bp = SE_BP[i]\n",
    "        if se_bp < threshold:\n",
    "            diff1[i] = 1\n",
    "        else:\n",
    "            diff1[i] = 0\n",
    "    return pd.DataFrame(diff1)\n",
    "\n",
    "def SLP_OVLP (diff, SE_SLP, SLP):\n",
    "    '''Method to determine significant changes in consecutive slopes\n",
    "    diff: column with the differences of consecutive slopes\n",
    "    SE_SLP: column with the standard error values of the slopes\n",
    "    SLP: column with slope values\n",
    "    returns: matrix with values of differences of slopes and nans when not valid'''\n",
    "    aux_diff = np.empty_like(diff)\n",
    "    range1 = []\n",
    "    range2 = []\n",
    "    ci = SE_SLP * 1.96\n",
    "    ci_low = (SLP - ci).to_numpy()\n",
    "    ci_high = (SLP + ci).to_numpy()\n",
    "    for j in range(ns-1):\n",
    "        range1 = np.insert(range1, 0, ci_low[j]) \n",
    "        range1 = np.insert(range1, 1, ci_high[j])  \n",
    "        range2 = np.insert(range2, 0, ci_low[j+1]) \n",
    "        range2 = np.insert(range2, 1, ci_high[j+1])  \n",
    "        if range2[0] < range1[0]:\n",
    "            if range2[1] < range1[0]:\n",
    "                aux_diff[j] = 1\n",
    "            else:\n",
    "                aux_diff[j] = 0\n",
    "        elif range2[0] > range1[1]:\n",
    "            aux_diff[j] = 1\n",
    "        else:\n",
    "            aux_diff[j] = 0\n",
    "        range1 = []\n",
    "        range2 = []\n",
    "    return pd.DataFrame(aux_diff)\n",
    "\n",
    "def plot1(x, y, keys, i):\n",
    "    xhat = np.linspace(min(x), max(x), num=10000)\n",
    "    yhat = my_pwlf_1.predict(xhat)\n",
    "    plt.subplot(int(i/3)+1, 3, i + 1) ### CHANGE 3  FOR NoC\n",
    "    plt.plot(x, y, 'o', label='Data')\n",
    "    plt.plot(xhat, yhat, '--', label='Model')\n",
    "    plt.title(str(keys[i]))\n",
    "    return\n",
    "\n",
    "def NEGATIVE_SLOPES (SLP):\n",
    "    '''Helps to identify negative slopes among the fitted models\n",
    "    Input data:\n",
    "    SLP: Slopes vector from the model i'''\n",
    "    zz=[]\n",
    "    aa = SLP\n",
    "    if any(k < 0 for k in aa):\n",
    "        if aa[aa.count()-1] < 0:\n",
    "            zz = 1\n",
    "        elif aa[0] < 0:\n",
    "            zz = 1\n",
    "        else:\n",
    "            zz = 0\n",
    "    return zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cbeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bp = 4 #Max number of breakpoints defined by factors such as rainy seasons and period of study.\n",
    "ee = pd.DataFrame()\n",
    "\n",
    "for i in range(np.array(keyscoord_lm).size-1): \n",
    "    print('Curve', i, 'is being analyzed now')\n",
    "    NoS = 2 \n",
    "    print('Number of segments:', NoS)\n",
    "    cum_lmmask = pd.DataFrame() \n",
    "    ssr_1 = []\n",
    "    aic = []\n",
    "    res1 = pd.DataFrame() \n",
    "    slp1 = pd.DataFrame() \n",
    "    serr = pd.DataFrame()\n",
    "    dd = pd.DataFrame()\n",
    "    slopes_matrix = pd.DataFrame()\n",
    "    yfit_matrix = pd.DataFrame()\n",
    "    bp_matrix = pd.DataFrame()\n",
    "    merged_C = pd.DataFrame()\n",
    "    stdrr_BP = pd.DataFrame()\n",
    "    m_index = 0\n",
    "    u=0\n",
    "    TT = True\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    while TT:\n",
    "        print('Number of segments now is:', NoS)\n",
    "        cum_lmmask = np.isfinite(auxiliar1[keyscoord_auxilar[i]])\n",
    "        y = auxiliar1[keyscoord_auxilar[i]][cum_lmmask] \n",
    "        x = auxiliar1.datenumber[cum_lmmask] \n",
    "        my_pwlf_1 = pwlf.PiecewiseLinFit(x, y); \n",
    "        res = my_pwlf_1.fit(NoS); \n",
    "        slopes = my_pwlf_1.calc_slopes()\n",
    "        std_err = my_pwlf_1.standard_errors(method = 'non-linear')  # standard errors\n",
    "        yfit = my_pwlf_1.predict(res)\n",
    "\n",
    "        # calculate the sum of squared residuals\n",
    "        ssr = my_pwlf_1.fit_with_breaks(my_pwlf_1.fit_breaks)\n",
    "        ssr_1.append(ssr)\n",
    "\n",
    "        #Plot the resulting fit\n",
    "        plot1(x, y, keyscoord_auxilar, i) #Change 0 for: 1: Number of curves and 2: i \n",
    "\n",
    "        #Number of parameters and data\n",
    "        nparam = my_pwlf_1.n_parameters + (NoS-1) +1\n",
    "        ndata = auxiliar1[keyscoord_auxilar[i]].count()\n",
    "\n",
    "        aa = np.append(std_err, res, axis = 0)\n",
    "        bb = np.append(aa, slopes, axis = 0)\n",
    "        cc = np.append(bb, yfit, axis = 0)\n",
    "        ccc = np.append(cc, ssr)\n",
    "        dd = pd.concat([dd,pd.DataFrame(ccc)], ignore_index=True, axis=1)\n",
    "\n",
    "        ee = pd.concat([ee,dd], ignore_index=True, axis=1)\n",
    "        if NoS <= max_bp:\n",
    "            print('Number of segments < max=4, run again with NoS+1')\n",
    "            NoS = NoS+1 \n",
    "            diff = pd.DataFrame()\n",
    "            dd = pd.DataFrame()\n",
    "        else:\n",
    "            TT=False\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save matrix of results so there is no need to run the models every time.\n",
    "#ee.to_csv('FILEWITHMODELS.csv')\n",
    "#If saved, read the file containing the models:\n",
    "#ee_1 = pd.DataFrame(pd.read_csv('FILEWITHMODELS.csv')).iloc[:,1:]\n",
    "#ee = ee_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3472991",
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 30 #30 days of Standard Error that gives me a C.I. of ~87%, 3 months at 1.5 times S.E. \n",
    "max_bp = 4 #Max number of breakpoints defined by rainy seasons and period of study.\n",
    "\n",
    "invalid_curves = []\n",
    "a2 = []\n",
    "a3 = []\n",
    "aic_v = []\n",
    "aic_300 = []\n",
    "aux1 = pd.DataFrame()\n",
    "aux2 = pd.DataFrame()\n",
    "aux3 = pd.DataFrame()\n",
    "aux4 = pd.DataFrame()\n",
    "aux6 = pd.DataFrame()\n",
    "aux20 = pd.DataFrame()\n",
    "BPS = pd.DataFrame()\n",
    "SLPS = pd.DataFrame()\n",
    "for j in range(np.array(keyscoord_lm).size-1):\n",
    "    for i in range(max_bp):\n",
    "        nb = i+1\n",
    "        ns = i+2\n",
    "        SLP = ee_1.iloc[3+ns+(2*nb):3+(2*nb)+(2*ns), i+max_bp*j].reset_index(drop=True)\n",
    "        indi = NEGATIVE_SLOPES(SLP)\n",
    "        if indi == 0:\n",
    "            a2 = np.append(a2,0)\n",
    "        else:\n",
    "            SE_BP = ee_1.iloc[1+ns:1+ns+nb, i+max_bp*j].reset_index(drop=True)\n",
    "            SE_SLP = ee_1.iloc[1:ns+1, i+max_bp*j].reset_index(drop=True)\n",
    "            BP = ee_1.iloc[2+nb+ns:2+ns+(2*nb), i+max_bp*j].reset_index(drop=True)\n",
    "            SLP = ee_1.iloc[3+ns+(2*nb):3+(2*nb)+(2*ns), i+max_bp*j].reset_index(drop=True)\n",
    "            diff = pd.DataFrame(SLP.to_numpy()[1:] - SLP.to_numpy()[:-1])\n",
    "\n",
    "            BP_C = BP_LOC(diff, thres, SE_BP) ## Check if the standard error of the breakpoints is higher than a threshold\n",
    "            SLP_C = SLP_OVLP (diff, SE_SLP, SLP) ## Check if the C.I. of the slopes overlap\n",
    "\n",
    "            if ((BP_C.all() == (BP_C/BP_C.fillna(1)).all()).all() and (SLP_C.all() == (SLP_C/SLP_C.fillna(1)).all()).all()):\n",
    "                a2 = np.append(a2,1)\n",
    "            else:\n",
    "                a2 = np.append(a2,0) #store if models comply with BP_C and SLP_C\n",
    "            BPS = pd.concat([BPS,BP_C], axis = 1, join='outer', ignore_index=True) #### TO CHECK ONLY\n",
    "            SLPS = pd.concat([SLPS,SLP_C], axis = 1, join='outer', ignore_index=True) #### TO CHECK ONLY\n",
    "        a3 = np.append(a3, a2) #### TO CHECK ONLY\n",
    "    \n",
    "    if a2.sum() == 0:\n",
    "        invalid_curves = np.append(invalid_curves, j) \n",
    "        a2 = []\n",
    "        aic_v = []\n",
    "    else:\n",
    "        for k in range(max_bp):\n",
    "            nb_k = k+1\n",
    "            ns_k = k+2\n",
    "            if a2[k] != 0:\n",
    "                ssr_min = ee_1.iloc[5+(2*ns_k)+(3*nb_k), k+max_bp*j]\n",
    "                ndata = auxiliar1[keyscoord_auxilar[j]].count()\n",
    "                nparam = 1+ns_k+nb_k\n",
    "                aic = aicc(ssr_min, nparam, ndata)\n",
    "                aic_v = np.append(aic_v, aic)\n",
    "            else:\n",
    "                aic_v = np.append(aic_v, 10000)\n",
    "        aic_300 = np.append(aic_300, aic_v)\n",
    "            \n",
    "        index_min = int(np.where(aic_v == np.amin(aic_v))[0])\n",
    "                \n",
    "        ns_index = index_min+2\n",
    "        nb_index = index_min+1\n",
    "        stdrr_BP = pd.DataFrame(np.array(ee_1.iloc[1+ns_index:1+ns_index+nb_index, index_min+(j*max_bp)].reset_index(drop=True)))\n",
    "        bp_matrix = pd.DataFrame(np.array(ee_1.iloc[1+nb_index+ns_index:3+ns_index+(2*nb_index), index_min+(j*max_bp)].reset_index(drop=True)))\n",
    "        num_bp = bp_matrix.size-2\n",
    "        slopes_matrix = pd.DataFrame(np.array(ee_1.iloc[3+ns_index+(2*nb_index):3+(2*nb_index)+(2*ns_index), index_min+(j*max_bp)].reset_index(drop=True)))\n",
    "        diff_slopes_matrix = pd.DataFrame(slopes_matrix.to_numpy()[1:] - slopes_matrix.to_numpy()[:-1])\n",
    "        yfit_matrix = pd.DataFrame(np.array(ee_1.iloc[3+(2*ns_index)+(2*nb_index):5+(2*ns_index)+(3*nb_index), index_min+(j*max_bp)].reset_index(drop=True)))\n",
    "\n",
    "        aux1 = pd.concat([aux1,bp_matrix], axis = 1, join='outer', ignore_index=True) #breakpoints\n",
    "        aux2 = pd.concat([aux2, slopes_matrix], axis = 1, join='outer', ignore_index=True) #Slopes\n",
    "        aux3 = pd.concat([aux3, yfit_matrix], axis = 1, join='outer', ignore_index=True) # y coordinates of the bp\n",
    "        aux4 = pd.concat([aux4, diff_slopes_matrix], axis = 1, join='outer', ignore_index=True) #slopes differences\n",
    "        aux6 = pd.concat([aux6, stdrr_BP], axis = 1, join='outer', ignore_index=True) #Breakpoints' standard errors\n",
    "        aux20 = np.append(aux20, num_bp) #Number of breakpoints per time series\n",
    "    \n",
    "        a2 = []\n",
    "        aic_v = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b25d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To obtain the remaining time series that could be fitted\n",
    "aux3_dropped = pd.DataFrame()\n",
    "aux1_dropped = pd.DataFrame()\n",
    "aux2_dropped = pd.DataFrame()\n",
    "aux6_dropped = pd.DataFrame()\n",
    "if pd.DataFrame(invalid_curves).shape[0] > 0:\n",
    "    keyscoord_lm = list(cum_lm.keys())\n",
    "    for ele in sorted(invalid_curves.astype(np.int), reverse = True):  \n",
    "        del keyscoord_lm[ele] \n",
    "\n",
    "aux3_dropped = aux3.copy()\n",
    "aux1_dropped = aux1.copy()\n",
    "aux2_dropped = aux2.copy()\n",
    "aux6_dropped = aux6.copy()\n",
    "print(len(keyscoord_lm))\n",
    "print(invalid_curves)\n",
    "print(np.array(invalid_curves).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for i in range(aux2_dropped.shape[1]):\n",
    "    a = len(aux2_dropped.loc[aux2_dropped[i] < 0])\n",
    "    b = np.append(b,a)\n",
    "\n",
    "print('The number of negative slopes in total is', b.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = aux4.shape[0]*aux4.shape[1]\n",
    "nan_total = aux4.isnull().sum().sum()\n",
    "num_bp = total - nan_total\n",
    "print('The total number of breakpoints is', num_bp)\n",
    "aux1_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of breakpoints\n",
    "aa_3 = []\n",
    "for i in range(aux1_dropped.shape[1]):\n",
    "    aa = aux1_dropped[i]\n",
    "    aa_1 = aa.isna().sum()\n",
    "    aa_2 = aa.shape[0] - aa_1 - 2\n",
    "    aa_3 = np.append(aa_3, aa_2)\n",
    "    \n",
    "    \n",
    "d = np.diff(np.unique(aa_3)).min()\n",
    "left_of_first_bin = aa_3.min() - float(d)/2\n",
    "right_of_last_bin = aa_3.max() + float(d)/2\n",
    "plt.hist(aa_3, np.arange(left_of_first_bin, right_of_last_bin + d, d))\n",
    "plt.show()    \n",
    "plt.hist(aa_3, range = (0,5), bins = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f17287",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=[]\n",
    "ee=[]\n",
    "gg=[]\n",
    "hh=[]\n",
    "count = 0\n",
    "for i in range(aux2_dropped.shape[1]):\n",
    "    aa = aux2_dropped[i]\n",
    "    if any(j < 0 for j in aa):\n",
    "        count = count+1\n",
    "        if aa[aa.count()-1] < 0:\n",
    "            bb = 1\n",
    "            cc = np.append(cc,bb)\n",
    "        elif aa[0] < 0:\n",
    "            ff = 1\n",
    "            gg = np.append(gg, ff)\n",
    "        else:\n",
    "            hh = np.append(hh,i)\n",
    "            dd = 1\n",
    "            ee = np.append(ee,dd)\n",
    "\n",
    "#print('The number of negative slopes at the end of the time series is', cc.size)\n",
    "#print('The number of negative slopes in the middle of the time series is', ee.size)\n",
    "#print('The number of negative slopes in the beginning of the time series is', gg.size)\n",
    "print('Total', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d60517",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1_d2 = aux1_dropped.copy()\n",
    "aux1_d2.columns = range(aux1_d2.shape[1])\n",
    "aux2_d2 = aux2_dropped.copy()\n",
    "aux2_d2.columns = range(aux2_d2.shape[1])\n",
    "aux3_d2 = aux3_dropped.copy()\n",
    "aux3_d2.columns = range(aux3_d2.shape[1])\n",
    "aux6_d2 = aux6_dropped.copy()\n",
    "aux6_d2.columns = range(aux6_d2.shape[1])\n",
    "aux1_d2.shape[1], aux2_d2.shape[1], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyscoord_lm_d2 = keyscoord_lm.copy()\n",
    "#for tt in sorted(hh.astype(int), reverse = True):  \n",
    "#        del keyscoord_lm_d2[tt] \n",
    "len(keyscoord_lm_d2), len(keyscoord_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160d2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show location of breakpoints within the data\n",
    "u=26\n",
    "plt.figure(figsize=(10,30))\n",
    "for i in range(10): #np.array(keyscoord_lm).size-1\n",
    "    #cum_lmmask[i] = np.isfinite(cum_lm[keyscoord_lm[i]])\n",
    "    y = cum_lm[keyscoord_lm_d2[i+u]] #values of cumulative curve\n",
    "    x = pd.to_datetime('1899-12-30') + pd.to_timedelta(cum_lm.datenumber,'D')\n",
    "    y_calc = aux3_d2[i+u]\n",
    "    x_calc = pd.to_datetime('1899-12-30') + pd.to_timedelta(aux1_d2[i+u],'D')\n",
    "    plt.subplot(10, 2, i + 1)\n",
    "    #plt.plot(x, y, 'ok', label='data')\n",
    "    plt.plot(x, auxiliar1[keyscoord_lm_d2[i+u]], 'ok', label='data', markersize=4)\n",
    "    plt.plot(x_calc, y_calc, 'o-r', label='fit')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Displacement (mm)')\n",
    "    plt.title(str(keyscoord_lm_d2[i+u]))\n",
    "    plt.xticks(rotation=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To obtain the differences of the slopes - Obtain accelerations/decelerations\n",
    "d2 = []\n",
    "for i in range(aux2_d2.shape[1]):\n",
    "    for j in range (aux2_d2.shape[0]-1):\n",
    "        if (aux2_d2.to_numpy()[1:,i][j] * aux2_d2.to_numpy()[:-1,i][j]) > 0:\n",
    "            d = aux2_d2.to_numpy()[1:,i][j] - aux2_d2.to_numpy()[:-1,i][j]\n",
    "            d2 = np.append(d2, d)\n",
    "        else:\n",
    "            d2 = np.append(d2, np.nan)\n",
    "\n",
    "differences = d2.reshape((aux2_d2.shape[0]-1, aux2_d2.shape[1]),order='F')\n",
    "print(pd.DataFrame(differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To obtain a matrix with indicators of accelerations/decelerations as 1/-1\n",
    "inv = np.empty((differences.shape))\n",
    "for i in range(aux2_d2.shape[0]-1):\n",
    "    for j in range(aux2_d2.shape[1]):\n",
    "        if differences[i,j] > 0:\n",
    "            inv[i,j] = 1\n",
    "        elif differences[i,j] < 0:\n",
    "            inv[i,j] = -1\n",
    "        else:\n",
    "            inv[i,j] = np.nan\n",
    "\n",
    "inv[np.isnan(inv)] = 0\n",
    "inv1 = np.vstack((np.zeros(aux1_d2.shape[1]),inv))\n",
    "inventory = pd.DataFrame(inv1)\n",
    "inventory.loc[len(inventory)] = 0\n",
    "#pd.concat([pd.DataFrame(np.zeros(aux1_dropped.shape[1])).T, inventory])\n",
    "print(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join breakpoint dates and acceleration/deceleration indicators\n",
    "df_breakpoints = pd.DataFrame(aux1_d2)\n",
    "df_slopes_ones = pd.DataFrame(inventory)\n",
    "df_inventory = pd.DataFrame()\n",
    "j=0\n",
    "for i in range(aux1_d2.shape[1]):\n",
    "    df_inventory[j] = df_breakpoints[i]\n",
    "    df_inventory[j+1] = df_slopes_ones[i]\n",
    "    j=j+2\n",
    "\n",
    "print(df_inventory)\n",
    "print(df_breakpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8dedcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add acc/dec to adjacent months based on the S.E. of the breakpoints\n",
    "from scipy.stats import norm\n",
    "aux7 = pd.DataFrame()\n",
    "aux7_new = pd.DataFrame()\n",
    "aux8 = pd.DataFrame()\n",
    "aux88 = pd.DataFrame()\n",
    "aux13 = pd.DataFrame()\n",
    "new_day_v = []\n",
    "b=[]\n",
    "gg=[]\n",
    "\n",
    "for i in range(np.array(keyscoord_lm_d2).size-1): #np.array(keyscoord_lm).size-1\n",
    "    aux7=pd.concat([df_breakpoints[i], \n",
    "                    df_slopes_ones[i],\n",
    "                (pd.to_datetime('1899-12-30') + pd.to_timedelta(aux1_d2[i],'D')).dt.day, aux6_d2[i]], \n",
    "                   ignore_index=True, axis=1)\n",
    "    a30=aux7[3].count()\n",
    "    for j in range(a30):\n",
    "        if (np.array(aux7)[j+1,2]!=15):\n",
    "            a = np.array(aux7)[j+1,2]-15\n",
    "            new_day = np.array(aux7)[j+1,0]-a\n",
    "            #to_check = pd.to_datetime('1899-12-30') + pd.to_timedelta(new_day,'D')).dt.day#should be 15\n",
    "        else:\n",
    "            new_day = np.array(aux7)[j+1,0]\n",
    "        b = np.append(b, new_day)\n",
    "    gg = np.insert(b, 0, np.array(aux7)[0,0])\n",
    "    new_day_v= np.append(gg, np.array(aux7)[1+a30,0]) ##bring to [] before finish\n",
    "    \n",
    "    aux7_new = pd.concat([pd.DataFrame(new_day_v), \n",
    "                     df_slopes_ones[i], aux6_d2[i]], ignore_index=True, axis=1)\n",
    "    for j in range(a30):\n",
    "        s = np.array(aux7_new)[j,2] #Standard error\n",
    "        p = norm(0, s).cdf(15) - norm(0, s).cdf(-15) #prob\n",
    "        p1 = (1-p)/2        \n",
    "        \n",
    "        a4 = np.array(aux7_new)[j+1,0]+30\n",
    "        a4_1 = np.array(aux7_new)[j+1,0]-30\n",
    "        a4_1_1 = np.array(aux7_new)[j+1,0]\n",
    "        a5 = np.array(aux7_new)[j+1,1]*p1\n",
    "        a5_1_1 = np.array(aux7_new)[j+1,1]*p\n",
    "        a6 = pd.Series([a4, a5])\n",
    "        a6_1 = pd.Series([a4_1, a5])\n",
    "        a6_1_1 = pd.Series([a4_1_1, a5_1_1])\n",
    "        a7 = pd.DataFrame([a6])\n",
    "        a7_1 = pd.DataFrame([a6_1])\n",
    "        a7_1_1 = pd.DataFrame([a6_1_1])\n",
    "        aux13 = pd.concat([aux13, a7, a7_1, a7_1_1], ignore_index=True).sort_values([0])\n",
    "        a10 = np.insert(np.array(aux13), 0, [np.array(aux7_new)[0,0], np.array(aux7_new)[0,1]])\n",
    "        a11 = np.append(a10, [np.array(aux7_new)[1+a30,0],np.array(aux7_new)[1+a30,1]])\n",
    "        \n",
    "    a12 = pd.DataFrame(a11.reshape((a30*3+2, 2)))\n",
    "    #aux88 = pd.concat([aux7, aux13], ignore_index=True)\n",
    "    aux8 = pd.concat([aux8, a12], ignore_index=True, axis=1)\n",
    "    \n",
    "    aux7_new = pd.DataFrame()\n",
    "    aux7 = pd.DataFrame()\n",
    "    aux13 = pd.DataFrame()\n",
    "    new_day_v=[]\n",
    "    b=[]\n",
    "    a11=[]\n",
    "            \n",
    "\n",
    "# #print(np.array(aux7))\n",
    "# print(aux8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New inventory with corrected number of acc/dec based on standard error values\n",
    "df_inventory_1 = pd.DataFrame()\n",
    "j=0\n",
    "for i in range (np.array(keyscoord_lm_d2).size-1):#np.array(keyscoord_lm).size-1\n",
    "    aux20 = aux8[[j,j+1]]\n",
    "    #aux21 = aux20.sort_values([0])\n",
    "    df_inventory_1 = pd.concat([df_inventory_1, aux20], ignore_index=True, axis=1)\n",
    "    j=j+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To separate acceleration and deceleration time series\n",
    "acc = []\n",
    "dec = []\n",
    "np_inventory = df_inventory_1.to_numpy()\n",
    "k=1\n",
    "for i in range(int(df_inventory_1.shape[1]/2)):\n",
    "    for j in range(df_inventory_1.shape[0]):\n",
    "        if ((df_inventory_1[i+k][j] > 0) and (df_inventory_1[i+k][j] != 0)):\n",
    "            acc.append(df_inventory_1[i+k-1][j])\n",
    "        elif ((df_inventory_1[i+k][j] < 0) and (df_inventory_1[i+k][j] != 0)):\n",
    "            dec.append(df_inventory_1[i+k-1][j])\n",
    "    k = k+1\n",
    "\n",
    "acc_calc = pd.to_datetime('1899-12-30') + pd.to_timedelta(acc,'D')\n",
    "dec_calc = pd.to_datetime('1899-12-30') + pd.to_timedelta(dec,'D')\n",
    "print(df_inventory_1)\n",
    "#print(acc_calc)\n",
    "#print(dec_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To put the acceleration and deceleration timeseries in the correct format\n",
    "acc_date = acc_calc.sort_values()\n",
    "dec_date = dec_calc.sort_values()\n",
    "acc_df = pd.DataFrame(np.ones(acc_date.shape))\n",
    "dec_df = pd.DataFrame(np.ones(dec_date.shape))\n",
    "\n",
    "acc_df['date'] = pd.to_datetime(acc_date)\n",
    "acc_df = acc_df.set_index('date')\n",
    "acc_df.columns = ['Accelerations']\n",
    "dec_df['date'] = pd.to_datetime(dec_date)\n",
    "dec_df = dec_df.set_index('date')\n",
    "dec_df.columns = ['Decelerations']\n",
    "\n",
    "acc_df_month = pd.DataFrame()\n",
    "dec_df_month = pd.DataFrame()\n",
    "acc_df_month['Accelerations'] = acc_df.Accelerations.resample('M', kind='period').sum()\n",
    "dec_df_month['Decelerations'] = dec_df.Decelerations.resample('M', kind='period').sum()\n",
    "\n",
    "joined = acc_df_month.join(dec_df_month, how = 'outer')\n",
    "ax = joined.plot.bar(rot=35, figsize=(15, 7), legend=True, color='br', xlabel='Date', ylabel='Number of observations', stacked=True)\n",
    "ax.set_xlabel('Date', fontsize = 18, fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel('Number of observations', fontsize = 18, fontdict=dict(weight='bold'))\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.locator_params(axis='x', nbins=25)\n",
    "plt.legend(loc='upper left', fontsize=16)\n",
    "\n",
    "#print(dec_df_month)\n",
    "#print(acc_df_month)\n",
    "#print(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5a7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
